#### zn - proportion of residential land zoned for lots over 25,000 sq.ft.
#### indus - proportion of non-retail business acres per town.
#### chas - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
#### nox - nitric oxides concentration (parts per 10 million)
#### rm - average number of rooms per dwelling
#### age - proportion of owner-occupied units built prior to 1940
#### dis - weighted distances to five Boston employment centres
#### rad - index of accessibility to radial highways
#### tax - full-value property-tax rate per $10,000
#### ptratio - pupil-teacher ratio by town
#### black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
#### lstat - % lower status of the population
#### medv - Median value of owner-occupied homes in $1000's
#### A curios feature of the dataset seems to be variable 14 ("medv"), which has the median value of ownder-occupied homes. Now that dataset has many values pegged exactly at 50k dollars which could be a case of censoring since a good deal of variability is seen at other median values of "medv".
```{r echo=T}
plot(Boston$med)
```
## Explore distributions of variables and their relationships
```{r echo=T}
# General summary of the dataset
summary(Boston)
# Matrix of the variables
pairs(Boston)
# Correlation matrix
cor_matrix <- cor(Boston) %>% round(2)
corrplot(cor_matrix, method = "circle", type = "upper",
cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```
#### In relation to variable distribution and how they might be related we could ask whether the crime rate in Boston is anyhow linked to the value of homes, i.e, whether the old Hollywood adage of crime-ridden areas of Boston & its dingy houses v/s affluent suburbs with lower crime rates hold true or not
```{r echo=T}
# Do crimes & locality correlate in Boston?
plot(crim ~ medv, data=Boston, main = "Do crimes & locality correlate", xlab = "median value of owner-occupied", ylab = "crim rate")
```
#### We see a rather interesting pattern here in relation to crime rate and housing prices. The crime rates for affluent neighbourhoods seem quite low in relation to lower/cheaper houses.
#### In this same manner we can explore more such variables against crime rates and investigate their distributions
```{r echo=T}
# How do other variables stack up against crime rates? Do we see patterns?
molten <- melt(Boston, id = "crim")
ggplot(molten, aes(x = value, y = crim))+
facet_wrap( ~ variable, scales = "free")+
geom_point()
```
#### From above figure we do see rather some interesting patterns especially related to crime rates.
## Standardize & observe
```{r echo=T}
# Centering and standardizing variables
boston_scaled <- scale(Boston)
# Summaries of the scaled variables
glimpse(boston_scaled)
summary(boston_scaled)
# Class of boston_scaled object
class(boston_scaled)
# Converting to data frame
boston_scaled <- as.data.frame(boston_scaled)
# Summary of the scaled crime rate
summary(Boston$crim)
# Quantile vector of 'crim'
bins <- quantile(boston_scaled$crim)
bins
# Categorical variable 'crime' from 'crim'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
# Tabulation of the new factor crime
table(crime)
# Removing original 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# Adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# Number of rows in the Boston dataset
n <- nrow(boston_scaled)
n
# Choosing randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
ind
# Training set
train <- boston_scaled[ind,]
# Test set
test <- boston_scaled[-ind,]
# Saving correct classes from test data
correct_classes <- test$crime
# Removing 'crime' variable from test data
test <- dplyr::select(test, -crime)
```
## LDA
```{r echo=T}
# Linear discriminant analysis
lda.fit <- lda(crime ~., data = train)
# lda.fit object
lda.fit
# Function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# Classes as numeric
classes <- as.numeric(train$crime)
# LDA results plotting
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```
## LDA results prediction
```{r echo=T}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
tab <- table(correct = correct_classes, predicted = lda.pred$class)
```
#### The prediction results as seen above in the diagonal tells us the actual accuracy of LDA. To summarise we draw a confusion matrix
```{r echo=T}
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
confusion(Pima.tr$type, PimaCV.lda$class)
```
# LDA results plotting
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.fit <- lda(crime ~., data = train)
# lda.fit object
lda.fit
# Function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# Classes as numeric
classes <- as.numeric(train$crime)
# LDA results plotting
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
lda.arrows(lda.fit, myscale = 4)
# LDA results plotting
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 4)
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
lda.arrows(lda.fit)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit)
# plot the lda results
p <- plot(lda.fit, dimen = 2, col = classes, pch = classes)
p + lda.arrows(lda.fit, myscale = 2)
p + lda.arrows(lda.fit, myscale = 2)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
tab <- table(correct = correct_classes, predicted = lda.pred$class)
tab
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No",
+ "Yes"))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
conCV1
tab[1,]
tab[2,]
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
tab
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
conCV1
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]),
tab[2, ]/sum(tab[2, ]),
tab[3, ]/sum(tab[3, ]),
tab[4, ]/sum(tab[4, ]))
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]),
tab[2, ]/sum(tab[2, ])) %>%
rbind (tab[3, ]/sum(tab[3, ]),
tab[4, ]/sum(tab[4, ]))
conCV <- rbind(conCV1, conCV2)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]),
tab[2, ]/sum(tab[2, ]))
conCV2 <- rbind (tab[3, ]/sum(tab[3, ]),
tab[4, ]/sum(tab[4, ]))
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
conCV2 <- rbind(tab[3, ]/sum(tab[3, ]), tab[4, ]/sum(tab[4, ]))
tab[3, ]
tab
# cross tabulate the results
tab <- table(correct = correct_classes, predicted = lda.pred$class)
tab
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
conCV2 <- rbind(tab[3, ]/sum(tab[3, ]), tab[4, ]/sum(tab[4, ]))
conCV <- rbind(conCV1, conCV2)
conCV
conCV3 <- rbind(conCV1, conCV2)
conCV3
pred1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
pred2 <- rbind(tab[3, ]/sum(tab[3, ]), tab[4, ]/sum(tab[4, ]))
Predict_accuracy <- rbind(pred1, pred2)
Predict_accuracy
rownames(Predict_accuracy) <- colnames(Predict_accuracy)
Predict_accuracy
require(caret)
confusionMatrix(correct_classes, lda.pred$class)
# Euclidean distance matrix
dist_eu <- dist(boston_scaled)
View(boston_scaled)
# Euclidean distance matrix
dist_eu <- dist(boston_scaled[,-crime])
View(boston_scaled)
View(Boston)
# Summary of the scaled crime rate
summary(Boston$crim)
# Removing original 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# Centering and standardizing variables
boston_scaled <- scale(Boston)
# Summaries of the scaled variables
glimpse(boston_scaled)
summary(boston_scaled)
# Class of boston_scaled object
class(boston_scaled)
# Converting to data frame
boston_scaled <- as.data.frame(boston_scaled)
# Summary of the scaled crime rate
summary(Boston$crim)
# Quantile vector of 'crim'
bins <- quantile(boston_scaled$crim)
bins
# Categorical variable 'crime' from 'crim'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
# Tabulation of the new factor crime
table(crime)
# Removing original 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# Number of rows in the Boston dataset
n <- nrow(boston_scaled)
n
# Choosing randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
ind
# Training set
train <- boston_scaled[ind,]
# Test set
test <- boston_scaled[-ind,]
# Saving correct classes from test data
correct_classes <- test$crime
# Removing 'crime' variable from test data
test <- dplyr::select(test, -crime)
# Saving correct classes from test data
correct_classes <- test$crime
# Removing 'crime' variable from test data
test <- dplyr::select(test, -crime)
View(test)
# Removing original 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# Centering and standardizing variables
boston_scaled <- scale(Boston)
# Summaries of the scaled variables
glimpse(boston_scaled)
summary(boston_scaled)
# Class of boston_scaled object
class(boston_scaled)
# Converting to data frame
boston_scaled <- as.data.frame(boston_scaled)
# Summary of the scaled crime rate
summary(Boston$crim)
# Quantile vector of 'crim'
bins <- quantile(boston_scaled$crim)
bins
# Categorical variable 'crime' from 'crim'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
# Tabulation of the new factor crime
table(crime)
# Removing original 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# Adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
ind <- sample(n,  size = n * 0.8)
ind
# Training set
train <- boston_scaled[ind,]
View(train)
# Test set
test <- boston_scaled[-ind,]
View(test)
# Saving correct classes from test data
correct_classes <- test$crime
# Removing 'crime' variable from test data
test <- dplyr::select(test, -crime)
boston_scaled[,-crime]
boston_scaled[,-"crime"]
# Euclidean distance matrix
dist_eu <- dist(boston_scaled[,-c("crime")])
boston_scaled[,-c("crime")]
boston_scaled[,-crime]
boston_scaled[-crime]
# Euclidean distance matrix
boston_scaled <- dplyr::select(boston_scaled, -crim)
View(boston_scaled)
View(boston_scaled)
# Euclidean distance matrix
boston_scaled <- dplyr::select(boston_scaled, -crime)
dist_eu <- dist(boston_scaled)
# Summary of the distances
summary(dist_eu)
# Manhattan distance matrix
dist_man <- dist(boston_scaled, method = 'manhattan')
# Summary of the distances
summary(dist_man)
# k-means clustering
km <-kmeans(boston_scaled, centers = 4)
# plot the Boston dataset with clusters
pairs(boston_scaled[6:10], col = km$cluster)
View(boston_scaled)
boston_scaled[6:10]
# plot the Boston dataset with clusters
pairs(boston_scaled[c(dis, medv, black, lstat, tax)], col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled[c("dis", "medv", "black", "lstat", "tax")], col = km$cluster)
km4 <-kmeans(boston_scaled, centers = 4)
# plot the Boston dataset with clusters
# For this, we choose 5 variables - dis, medv, black, lstat and tax
pairs(boston_scaled[c("dis", "medv", "black", "lstat", "tax")], col = km4$cluster)
# k-means clustering with 3
km3 <-kmeans(boston_scaled, centers = 3)
# plot the Boston dataset with clusters
# For this, we choose 5 variables - dis, medv, black, lstat and tax
pairs(boston_scaled[c("dis", "medv", "black", "lstat", "tax")], col = km3$cluster)
set.seed(100)
# Compute and plot cluster addition & variance explained for k = 2 to k = 15.
k.max <- 15
data <- boston_scaled
clust_var <- sapply(1:k.max,
function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
clust_var
plot(1:k.max, clust_var,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
set.seed(100)
# Compute and plot cluster addition & variance explained for k = 2 to k = 15.
k.max <- 15
data <- boston_scaled
clust_TSS <- sapply(1:k.max,
function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
clust_TSS
plot(1:k.max, clust_var,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
# plot the Boston dataset with clusters
# For this, we choose 5 variables - dis, medv, black, lstat and tax
ggpairs(boston_scaled[c("dis", "medv", "black", "lstat", "tax")], col = km4$cluster)
# plot the Boston dataset with clusters
# For this, we choose 5 variables - dis, medv, black, lstat and tax
ggpairs(boston_scaled[c("dis", "medv", "black", "lstat", "tax")], col = km4$cluster)
# plot the Boston dataset with clusters
# For this, we choose 5 variables - dis, medv, black, lstat and tax
pairs(boston_scaled[c("dis", "medv", "black", "lstat", "tax")], col = km4$cluster)
# k-means clustering with 4
km_bonus <-kmeans(boston_scaled, centers = 4)
km4$cluster
km_bonus
# Linear discriminant analysis
lda.fit.bonus <- lda(km_bonus$cluster ~., data = train)
View(train)
# Linear discriminant analysis
lda.fit.bonus <- lda(km_bonus$cluster ~., data = train)
nrow(train)
nrow(km_bonus$cluster)
length(km_bonus$cluster)
km_bonus$cluster
# Linear discriminant analysis
cluster_target <- km_bonus$cluster %in% train
# Linear discriminant analysis
cluster_target <- km_bonus$cluster[km_bonus$cluster %in% train, ]
# Linear discriminant analysis
cluster_target <- km_bonus$cluster[, km_bonus$cluster %in% train ]
km_bonus$cluster
as.matrix(km_bonus$cluster)
# Linear discriminant analysis
cluster_target <- km_bonus$cluster[ km_bonus$cluster %in% rownames(train), ]
cluster_target <- km_bonus$cluster[ rownames(mat) %in% rownames(train), ]
# Linear discriminant analysis
mat <- as.matrix(km_bonus$cluster)
cluster_target <- km_bonus$cluster[ rownames(mat) %in% rownames(train), ]
cluster_target <- train[ rownames(mat) %in% rownames(train), ]
View(cluster_target)
cluster_target <- train[ mat %in% train, ]
View(cluster_target)
cluster_target <- train[ mat %in% rownames(train), ]
View(cluster_target)
# Linear discriminant analysis
mat <- as.matrix(km_bonus$cluster)
cluster_target <- train[ mat %in% rownames(train), ]
cluster_target <- train[ rownames(mat) %in% rownames(train), ]
nrow(train)
View(cluster_target)
lda.fit.bonus <- lda(km_bonus$cluster ~., data = cluster_target)
View(cluster_target)
lda.fit.bonus <- lda(cluster_target ~., data = train)
mat <- as.matrix(km_bonus$cluster)
View(mat)
View(train)
cluster_target <- cluster_target[ rownames(mat) %in% rownames(train), ]
View(cluster_target)
km_bonus <-kmeans(boston_scaled, centers = 4)
# Linear discriminant analysis
mat <- as.matrix(km_bonus$cluster)
cluster_target <- mat[ rownames(mat) %in% rownames(train), ]
cluster_target
length(cluster_target)
lda.fit.bonus <- lda(cluster_target ~., data = train)
# Plot the lda results
plot(lda.fit.bonus, dimen = 2, col = classes, pch = classes)
plot(lda.fit.bonus, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
View(train)
# target classes as numeric
classes <- as.numeric(cluster_target)
# Plot the lda results
plot(lda.fit.bonus, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
plot(lda.fit.bonus, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 4)
# Plot the lda results
plot(lda.fit.bonus, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
install.packages("plotly")
library("plotly")
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
?plot_ly
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type = 'scatter3d', mode = 'markers', color = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type = 'scatter3d', mode = 'markers', color = cluster_target)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv",
stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv",
stringsAsFactors = F, na.strings = "..")
str(hd)
summary(hd)
str(gii)
summary(gii)
View(hd)
colnames(hd) <- c("Rank" , "Country", "HDindex" , "Life.expectancy", "Exp.school.years",
"Mean.school.yrs", "GNIncome", "GNI.minus.Rank")
View(gii)
# Variable renaming for both datasets
colnames(hd) <- c("Rank" , "Country", "HDindex" , "Life.expectancy", "Exp.school.years",
"Mean.school.yrs", "GNIncome", "GNI.minus.Rank")
colnames(gii) <- c("Rank" , "Country", "GIindex" , "Mat.Mort.Rate", "Adol.Birth.Rate",
"Rep.Parliament(%)", "Pop.sec.edu(Female)", "Pop.sec.edu(Male)",
"Lab.part.Rate(Female)", "Lab.part.Rate(Male)")
colnames(hd) <- c("Rank" , "Country", "HDindex" , "Life.expectancy", "Exp.school.years",
"Mean.school.yrs", "GNIncome", "GNI.minus.Rank")
colnames(gii) <- c("Rank" , "Country", "GIindex" , "Mat.Mort.Rate", "Adol.Birth.Rate",
"Rep.Parliament(%)", "Pop.sec.edu(Female)", "Pop.sec.edu(Male)",
"Lab.part.Rate(Female)", "Lab.part.Rate(Male)")
View(gii)
colnames(gii) <- c("Rank" , "Country", "GIindex" , "Mat.Mort.Rate", "Adol.Birth.Rate",
"Rep.Parliament(%)", "FSec.edu", "MSec.edu",
"FLab.Rate", "MLab.Rate")
gii <- gii %>% mutate((Edu.F2M = FSec.edu/MSec.edu), (Lab.F2M = FLab.Rate/MLab.Rate))
gii <- gii %>% mutate(gii, (Edu.F2M = FSec.edu/MSec.edu), (Lab.F2M = FLab.Rate/MLab.Rate))
gii <- gii %>% mutate((gii$Edu.F2M = FSec.edu/MSec.edu), (gii$Lab.F2M = FLab.Rate/MLab.Rate))
gii <- gii %>% as_tibble() %>% mutate((Edu.F2M = FSec.edu/MSec.edu),
(Lab.F2M = FLab.Rate/MLab.Rate))
View(gii)
gii <- gii %>% mutate(Edu.F2M = FSec.edu/MSec.edu,
Lab.F2M = FLab.Rate/MLab.Rate)
View(gii)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv",
stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv",
stringsAsFactors = F, na.strings = "..")
# Data description & summary
str(hd)
summary(hd)
str(gii)
summary(gii)
# Variable renaming for both datasets
colnames(hd) <- c("Rank" , "Country", "HDindex" , "Life.expectancy", "Exp.school.years",
"Mean.school.yrs", "GNIncome", "GNI.minus.Rank")
colnames(gii) <- c("Rank" , "Country", "GIindex" , "Mat.Mort.Rate", "Adol.Birth.Rate",
"Rep.Parliament(%)", "FSec.edu", "MSec.edu",
"FLab.Rate", "MLab.Rate")
gii <- gii %>% mutate(Edu.F2M = FSec.edu/MSec.edu,
Lab.F2M = FLab.Rate/MLab.Rate)
View(gii)
View(gii)
human <- inner_join(hd, gii, by = "Country")
View(human)
setwd("~/Documents/GitHub/IODS-project")
getwd()
write.csv(human, "~/Documents/GitHub/IODS-project/data/human.csv",
row.names = FALSE)
?write.csv
write.table(human, "~/Documents/GitHub/IODS-project/data/human.txt",
row.names = FALSE)
write.table(human, "~/Documents/GitHub/IODS-project/data/human.txt",
sep = ",", row.names = FALSE)
write.table(human, "~/Documents/GitHub/IODS-project/data/human.txt",
sep = "/t", row.names = FALSE)
write.table(human, "~/Documents/GitHub/IODS-project/data/human.txt",
sep = "\t", row.names = FALSE)
